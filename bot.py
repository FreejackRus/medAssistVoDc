# medical_ollama.py  ‚Äî  —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å Ollama (REST API)
from __future__ import annotations

import os
import re
from pathlib import Path
from typing import Dict, List, TextIO
import fitz
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import ollama  # pip install ollama

# ------------------------- CONFIG -------------------------
OLLAMA_MODEL: str = "hf.co/unsloth/Magistral-Small-2506-GGUF:Q6_K"          # —Å–Ω–∞—á–∞–ª–∞ ¬´ollama pull <name>¬ª
EMBED_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE: Path = Path("docs/services.xlsx")
CHUNK_SIZE: int = 1_000
CHUNK_OVERLAP: int = 200
TOP_K: int = 50
MAX_INPUT_TOK: int = 128_000
# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
SECTION_PATTERNS = [
    re.compile(r"^(\d+(?:\.\d+)*)\s+([^\n]+)", re.MULTILINE),  # 1.1, 2.3.4, ‚Ä¶ + title
    re.compile(r"^([IVX]+)\.\s+([^\n]+)", re.MULTILINE),       # I., II., III. + title
    re.compile(r"^([–ê-–Ø][–∞-—è]+)\s*\n", re.MULTILINE),         # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
    re.compile(r"^(\d+)\.\s+([–ê-–Ø–Å][^\n]+)", re.MULTILINE),   # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
    re.compile(r"^([–ê-–Ø–Å\s]{3,})\n", re.MULTILINE),           # –ó–ê–ì–û–õ–û–í–ö–ò –ó–ê–ì–õ–ê–í–ù–´–ú–ò
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏–∞–≥–Ω–æ–∑–∞
DIAGNOSIS_PATTERNS = [
    re.compile(r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç[^\n]*)", re.I),
    re.compile(r"(–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è[^\n]*)", re.I),
    re.compile(r"(–ê—Å—Ç–º–∞[^\n]*)", re.I),
    re.compile(r"(–ê—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è[^\n]*)", re.I),
    re.compile(r"(–ò—à–µ–º–∏—á–µ—Å–∫–∞—è –±–æ–ª–µ–∑–Ω—å —Å–µ—Ä–¥—Ü–∞[^\n]*)", re.I),
    re.compile(r"(–•—Ä–æ–Ω–∏—á–µ—Å–∫–∞—è –æ–±—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –±–æ–ª–µ–∑–Ω—å –ª–µ–≥–∫–∏—Ö[^\n]*)", re.I),
    re.compile(r"(–ü–Ω–µ–≤–º–æ–Ω–∏—è[^\n]*)", re.I),
    re.compile(r"(–ò–Ω—Ñ–∞—Ä–∫—Ç –º–∏–æ–∫–∞—Ä–¥–∞[^\n]*)", re.I),
    re.compile(r"(–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏[^\n]*)", re.I),
]
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self) -> None:
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        self.db = self._load_or_build_index()

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": "cpu"},  # –º–æ–∂–Ω–æ "cuda", –µ—Å–ª–∏ –µ—Å—Ç—å
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache",
        )

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent),
                self.embeddings,
                index_name=idx_path.stem,
                allow_dangerous_deserialization=True,
            )

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(
                page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                metadata={
                    "source": "services",
                    "id": str(row["ID"]),
                    "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]),
                },
            )
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF PROCESSING ----------
    def _extract_text_with_fallback(self, pdf_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        try:
            # –ú–µ—Ç–æ–¥ 1: PyMuPDF (fitz)
            doc = fitz.open(pdf_path)
            text_parts = []
            
            for page_num, page in enumerate(doc):
                try:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
                    text = page.get_text("text")
                    
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    if len(text.strip()) < 100:
                        print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
                        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å OCR, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        # text = self._ocr_page(page)
                    
                    text_parts.append(text)
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
                    continue
            
            doc.close()
            full_text = "\n".join(text_parts)
            
            if len(full_text.strip()) < 500:
                print("–ò–∑–≤–ª–µ—á–µ–Ω–æ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥")
                return self._extract_with_langchain(pdf_path)
                
            return full_text
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ PyMuPDF: {e}")
            return self._extract_with_langchain(pdf_path)

    def _extract_with_langchain(self, pdf_path: str) -> str:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LangChain"""
        try:
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            return "\n".join(doc.page_content for doc in documents)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LangChain PDF loader: {e}")
            return ""

    def _extract_diagnosis_name(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 3000 —Å–∏–º–≤–æ–ª–∞—Ö
        search_text = text[:3000]
        
        for pattern in DIAGNOSIS_PATTERNS:
            match = pattern.search(search_text)
            if match:
                diagnosis = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                diagnosis = re.sub(r'\s+', ' ', diagnosis)
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥–∏–∞–≥–Ω–æ–∑: {diagnosis}")
                return diagnosis
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        lines = search_text.split('\n')
        for line in lines[:20]:  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
            line = line.strip()
            if len(line) > 10 and any(word in line.lower() for word in 
                                    ['—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', '–ø—Ä–æ—Ç–æ–∫–æ–ª', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '–∞–ª–≥–æ—Ä–∏—Ç–º']):
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {line}")
                return line
        
        print("‚ö†Ô∏è –î–∏–∞–≥–Ω–æ–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"

    def _parse_sections_advanced(self, text: str) -> Dict[str, str]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        sections = {}
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in SECTION_PATTERNS:
            splits = pattern.split(text)
            if len(splits) > 3:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª—ã
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern.pattern}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
                for i in range(1, len(splits), 3):
                    if i + 2 < len(splits):
                        number = splits[i].strip()
                        title = splits[i + 1].strip()
                        body = splits[i + 2].strip()
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á
                        if number and title:
                            key = f"{number} {title}".strip()
                        elif title:
                            key = title
                        else:
                            continue
                            
                        # –û—á–∏—â–∞–µ–º —Ç–µ–ª–æ —Ä–∞–∑–¥–µ–ª–∞
                        if body and len(body) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                            sections[key] = body
                
                if sections:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω
                    break
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π —Ä–∞–∑–¥–µ–ª
        if not sections:
            print("‚ö†Ô∏è –†–∞–∑–¥–µ–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç")
            sections["guidelines"] = text
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {len(sections)}")
        for key in list(sections.keys())[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {key}")
        
        return sections

    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        """
        Reads a Russian clinical guideline PDF and returns a dict:
            {"1.1 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "...", "1.2 –≠—Ç–∏–æ–ª–æ–≥–∏—è": "...", ...}
        Keeps Cyrillic headings intact.
        """
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å fallback –º–µ—Ç–æ–¥–∞–º–∏
        full_text = self._extract_text_with_fallback(pdf_path)
        
        if not full_text or len(full_text.strip()) < 100:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF")
            return {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞
        self.diagnosis_name = self._extract_diagnosis_name(full_text)
        print(f"‚úÖ –î–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}")

        # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–¥–µ–ª—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
        sections = self._parse_sections_advanced(full_text)
        
        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª "guidelines", –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ-–¥—Ä—É–≥–æ–º—É
        if len(sections) == 1 and "guidelines" in sections:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            text = sections["guidelines"]
            keyword_sections = {}
            
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = [
                ("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", r"(–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–¥–µ—Ñ–∏–Ω–∏—Ü–∏—è)"),
                ("–≠—Ç–∏–æ–ª–æ–≥–∏—è", r"(—ç—Ç–∏–æ–ª–æ–≥–∏—è|–ø—Ä–∏—á–∏–Ω—ã|—Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞)"),
                ("–ü–∞—Ç–æ–≥–µ–Ω–µ–∑", r"(–ø–∞—Ç–æ–≥–µ–Ω–µ–∑|–º–µ—Ö–∞–Ω–∏–∑–º)"),
                ("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", r"(–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è|—Ç–∏–ø—ã|–≤–∏–¥—ã)"),
                ("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", r"(–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¥–∏–∞–≥–Ω–æ–∑|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)"),
                ("–õ–µ—á–µ–Ω–∏–µ", r"(–ª–µ—á–µ–Ω–∏–µ|—Ç–µ—Ä–∞–ø–∏—è|–ø—Ä–µ–ø–∞—Ä–∞—Ç—ã)"),
                ("–ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞", r"(–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞|–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ)"),
                ("–ü—Ä–æ–≥–Ω–æ–∑", r"(–ø—Ä–æ–≥–Ω–æ–∑|–∏—Å—Ö–æ–¥)"),
            ]
            
            for section_name, pattern in keywords:
                match = re.search(f"({pattern}.*?)(?=({'|'.join([p[1] for p in keywords])})|$)", 
                                text, re.I | re.DOTALL)
                if match:
                    keyword_sections[section_name] = match.group(1).strip()
            
            if keyword_sections:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {len(keyword_sections)}")
                sections = keyword_sections

        return sections

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text: str, max_tok: int) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    def find_services(self, query: str, k: int = TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k * 3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO) -> None:
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(
            f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services
        )


        content = sections.get("guidelines", "")
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        system_prompt = (
            "--think=false \n"
            "–¢—ã ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            "–í–∫–ª—é—á–∏ –í–°–ï –¥–∞–Ω–Ω—ã–µ: —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω—é–∞–Ω—Å—ã. "
            "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —É–∫–∞–∂–∏ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è."
            "–ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–∞–¥—Ü–∏—è—Ö —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ ¬´–Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å¬ª / ¬´–Ω–µ –¥–µ–ª–∞—Ç—å¬ª ‚Äî –≤–∫–ª—é—á–∏ —ç—Ç–æ. "
            " –ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –æ–¥–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏—Å–∫–ª—é—á–∞–µ—Ç –¥—Ä—É–≥–æ–µ - –≤–∫–ª—é—á–∏ —ç—Ç–æ –∏ —Ä–∞—Å–ø–∏—à–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ—Å—Ç —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∞—Ö–∞—Ä–Ω–æ–º –¥–∏–∞–±–µ—Ç–µ."
            "–ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã. "
            "–í—Å–µ –æ—Ç–≤–µ—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π "
            "–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ c —à–∏—Ä–æ–∫–∏–º–∏ –æ–±—å—è—Å–Ω–µ–Ω–∏—è–º–∏, –º–µ—Ç—Ä–∏–∫–∞–º–∏  –∏ –ø–æ –¥–µ–ª—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –ª–µ—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞."
            "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω—å –±—ã—Ç—å –±–µ–∑ –≤–≤–æ–¥–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –æ—Ç —Ç–µ–±—è."
        )
        user_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ö–ª–∏–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ—Å—Ç—Ä–æ–π **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π –∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π** –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—é ¬´{self.diagnosis_name}¬ª.

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –í–∫–ª—é—á–∏ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, —Å—Ä–æ–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è.
        - –£–∫–∞–∂–∏ **–≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏**, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.
        - –í–∫–ª—é—á–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –ø—Ä—è–º–æ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –∑–∞–ø—Ä–µ—Ç—ã/–æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º:

        ### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
        - –≠–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è, —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        - –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        ### II. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (–∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä)
        2. –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–≤—Å–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –Ω–æ—Ä–º—ã, –∏—Å–∫–ª—é—á–µ–Ω–∏—è)
        3. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        4. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        5. –ö–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å–∫–ª—é—á–∞—é—Ç –¥—Ä—É–≥–∏–µ –∏ –≤ –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)

        ### III. –õ–µ—á–µ–Ω–∏–µ
        1. –ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
        2. –ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è (–ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Ä–µ–∂–∏–º, –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏)
        3. –ü—Ä–æ—Ü–µ–¥—É—Ä—ã / –æ–ø–µ—Ä–∞—Ü–∏–∏ / –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (—á–∞—Å—Ç–æ—Ç–∞, —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)

        ### IV. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        - –°—Ä–æ–∫–∏, –º–µ—Ç–æ–¥—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–∞

        ### V. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞
        - –û—Å–ª–æ–∂–Ω–µ–Ω–∏—è, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è, —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        - –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –∏ –¥–∏—Å–ø–∞–Ω—Å–µ—Ä–∏–∑–∞—Ü–∏—è

        ---

        **–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
        {content}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
                think=False,
                options={"temperature": 0.6, "top_p": 0.95}

            )
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
        except Exception as e:
            print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama:", e)

    # ---------- RUN ----------
    def run(self):
        if not self.db:
            print("‚ùå Index not loaded")
            return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit", "q"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue

            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            outfile = Path("testSec.txt")
            with outfile.open("w", encoding="utf-8") as f:
                for key, text in sections.items():
                    f.write(f"--- {key.upper()} ---\n{text}\n\n")
            print("‚úÖ Raw sections saved ‚Üí", outfile.absolute())
            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"test.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("\n‚úÖ Saved ‚Üí", outfile.absolute())


# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("‚ùå Fatal:", e)