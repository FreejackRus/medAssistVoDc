# medical_qwen.py
from __future__ import annotations

import os, re, threading, math, gc, json
from pathlib import Path
from typing import Dict, List, TextIO, Tuple

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from torch import cuda
from gpu_monitor import print_vram   # optional

# ---------- CONFIG ----------
HF_MODEL_NAME   = "moonshotai/Kimi-K2-Instruct"
EMBED_MODEL     = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE   = Path("docs/services.xlsx")
CHUNK_SIZE      = 1_000
CHUNK_OVERLAP   = 200
MAX_NEW_TOKENS  = 10_048
MAX_INPUT_TOK   = 128_000
TOP_K           = 50
# ---------------------------

class MedicalAssistant:
    def __init__(self):
        self._stop_monitor = threading.Event()
        self.device = "cuda" if cuda.is_available() else "cpu"
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        self.tokenizer, self.model = self._load_llm()
        self.db = self._load_or_build_index()

    # ---------- LLM ----------
    from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

    def _load_llm(self):
        print("‚åõ Loading Qwen 7B with YaRN (RoPE scaling)...")
        tok = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)

        # === YaRN RoPE scaling ===
        rope_kwargs = {
            "rope_scaling": {
                "type": "yarn",  # YaRN
                "factor": 4.0,  # 4x –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (32k ‚Üí ~128k)
                "original_max_position_embeddings": 32768
            }
        }

        mdl = AutoModelForCausalLM.from_pretrained(
            HF_MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            **rope_kwargs
        )
        print("‚úÖ YaRN-enabled model ready")
        return tok, mdl

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        """Reuse disk cache so 1 583 rows load instantly."""
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": self.device},
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache")   # <- 40 MB on disk

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        """Load FAISS index from disk if present, else build."""
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent), self.embeddings,
                index_name=idx_path.stem, allow_dangerous_deserialization=True)

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                     metadata={"source": "services", "id": str(row["ID"]), "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"])})
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF ----------
    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found"); return {}
        pages = PyPDFLoader(pdf_path).load_and_split()
        full_text = "\n".join(p.page_content for p in pages)

        title = re.search(r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n", full_text[:2000], re.I)
        self.diagnosis_name = (title.group(1) if title else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ").strip()
        print(f"‚úÖ Diagnosis: {self.diagnosis_name}")

        def grab(txt, start, end):
            s = re.search(start, txt, re.I)
            if not s: return ""
            e = re.search(end, txt[s.end():], re.I)
            return txt[s.start(): s.end() + (e.start() if e else len(txt))].strip()

        return {
            "diagnosis": grab(full_text, "–¥–∏–∞–≥–Ω–æ–∑", "–ª–µ—á–µ–Ω–∏–µ|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"),
            "treatment": grab(full_text, "–ª–µ—á–µ–Ω–∏–µ", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è"),
            "monitoring": grab(full_text, "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è"),
            "complications": grab(full_text, "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"),
        }

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text, max_tok) -> str:
        max_chars = int(max_tok * 4.5)
        return text[:max_chars].rsplit("\n", 1)[0] if len(text) > max_chars else text

    def find_services(self, query, k=TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k*3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO):
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services)

        prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –≤—Ä–∞—á-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.\n"
            f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —É—Å–ª—É–≥–∏:\n{services_list}\n\n"
            "–§–æ—Ä–º–∞—Ç:\nü©∫ –≠—Ç–∞–ø—ã ‚Ä¶\nüîç –û–ø–∏—Å–∞–Ω–∏–µ ‚Ä¶\n–î–ï–ô–°–¢–í–ò–ï:\n"
            "- –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏\n- –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞\n- –ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è\n"
            "- –£—Å–ª–æ–≤–∏—è –ø–µ—Ä–µ—Ö–æ–¥–∞\n- K–ù: [0-1]\n\n"
        )
        content = "\n\n".join(f"--- {k.upper()} ---\n{v}" for k, v in sections.items() if v)
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        messages = [{"role": "user", "content": prompt + content}]
        chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
        inputs = self.tokenizer(chat, return_tensors="pt").to(self.model.device)

        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        with torch.no_grad():
            generated = self.model.generate(
                **inputs, max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.3, do_sample=True,
                streamer=streamer,
                pad_token_id=self.tokenizer.eos_token_id)

        answer = self.tokenizer.decode(
            generated[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        file.write(answer)

    # ---------- RUN ----------
    def run(self):
        if not (self.tokenizer and self.model and self.db):
            print("‚ùå Not initialized"); return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit"}: break
            if not os.path.exists(pdf):
                print("‚ùå File not found"); continue
            sections = self.load_guidelines(pdf)
            if not sections: continue

            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏_{safe}.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("‚úÖ Saved ‚Üí", outfile.absolute())

if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except Exception as e:
        print("‚ùå Fatal:", e)
    finally:
        input("Press Enter to exit")