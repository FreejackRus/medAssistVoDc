# medical_ollama.py  ‚Äî  —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å Ollama (REST API)
from __future__ import annotations

import os
import re
from pathlib import Path
from typing import Dict, List, TextIO
import fitz
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import ollama  # pip install ollama

# ------------------------- CONFIG -------------------------
OLLAMA_MODEL: str = "hf.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:Q6_K"          # —Å–Ω–∞—á–∞–ª–∞ ¬´ollama pull <name>¬ª
EMBED_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE: Path = Path("docs/services.xlsx")
CHUNK_SIZE: int = 1_000
CHUNK_OVERLAP: int = 200
TOP_K: int = 50
MAX_INPUT_TOK: int = 128_000
SECTION_PATTERN = re.compile(
    r"^(\d+(?:\.\d+)*)\s+([^\n]+)",  # 1.1, 2.3.4, ‚Ä¶ + title
    re.MULTILINE,
)
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self) -> None:
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        self.db = self._load_or_build_index()

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": "cpu"},  # –º–æ–∂–Ω–æ "cuda", –µ—Å–ª–∏ –µ—Å—Ç—å
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache",
        )

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent),
                self.embeddings,
                index_name=idx_path.stem,
                allow_dangerous_deserialization=True,
            )

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(
                page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                metadata={
                    "source": "services",
                    "id": str(row["ID"]),
                    "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]),
                },
            )
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF ----------
    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        """
        Reads a Russian clinical guideline PDF and returns a dict:
            {"1.1 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "...", "1.2 –≠—Ç–∏–æ–ª–æ–≥–∏—è": "...", ...}
        Keeps Cyrillic headings intact.
        """
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}

        doc = fitz.open(pdf_path)
        full_text = "\n".join(page.get_text("text") for page in doc)
        doc.close()

        # Grab diagnosis name from the first page
        title = re.search(
            r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n",
            full_text[:2000],
            re.I,
        )
        self.diagnosis_name = (
            title.group(1).strip() if title else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
        )
        print(f"‚úÖ Diagnosis: {self.diagnosis_name}")

        # Split by headings
        sections = {}
        splits = SECTION_PATTERN.split(full_text)
        # split returns [prefix, num1, title1, body1, num2, title2, body2, ...]
        for i in range(1, len(splits), 3):
            number, title, body = splits[i], splits[i + 1], splits[i + 2]
            key = f"{number} {title}".strip()
            sections[key] = body.strip()

        return sections

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text: str, max_tok: int) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    def find_services(self, query: str, k: int = TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k * 3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO) -> None:
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(
            f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services
        )


        content = sections.get("guidelines", "")
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        system_prompt = (
            "--think=false \n"
            "–¢—ã ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π PDF-–¥–æ–∫—É–º–µ–Ω—Ç. "
            "–¢—ã –Ω–µ –∏–º–µ–µ—à—å –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É, –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—à—å —Å—Å—ã–ª–∫–∏, –Ω–µ —Å–æ–∑–¥–∞—ë—à—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—à—å —Å –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ. "
            "–í—Å–µ –æ—Ç–≤–µ—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ PDF. "
            "–ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ–±—â–∏: ¬´–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç¬ª. "
            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –ª–µ—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞."
        )
        user_prompt = f"""
        –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—é ¬´{self.diagnosis_name}¬ª.  
        –ù–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ –Ω–∞–ø–∏—à–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–ª–µ–µ.
        –£—á—Ç–∏ –≤—Å–µ –Ω—å—é–∞–Ω—Å—ã, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –≤–∞–∂–Ω—ã–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–∏–∞–≥–æ—Å–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Å–∞—Ö–∞—Ä–Ω–æ–º –¥–∏–∞–±–µ—Ç–µ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –Ω–µ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–µ—Å—Ç —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –≥–ª—é–∫–æ–∑–µ.

        –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
        ### –ü–æ–¥—Ä–æ–±–Ω—ã–π –ê–ª–≥–æ—Ä–∏—Ç–º –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –õ–µ—á–µ–Ω–∏—è {self.diagnosis_name}

        #### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–∞
        - –∫—Ä–∞—Ç–∫–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–≥–Ω–æ–∑–∞
        - —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞ –∏ –ø—Ä–∏—á–∏–Ω—ã
        - –∫–ª—é—á–µ–≤—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        #### II. –≠—Ç–∞–ø—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ  
        2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã  
        3. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

        #### III. –¢–∞–∫—Ç–∏–∫–∞ –ª–µ—á–µ–Ω–∏—è
        1. –ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è  
        2. –ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è  
        3. –ü—Ä–æ—Ü–µ–¥—É—Ä—ã / –æ–ø–µ—Ä–∞—Ü–∏–∏  
        4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

        #### IV. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ

        #### V. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞

        #### VI. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (FAQ, –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏)

        ---

        **–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ pdf:**
        {content}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
                think=False,
                options={"temperature": 0.6, "top_p": 0.95}

            )
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
        except Exception as e:
            print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama:", e)

    # ---------- RUN ----------
    def run(self):
        if not self.db:
            print("‚ùå Index not loaded")
            return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit", "q"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue

            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            outfile = Path("testSec.txt")
            with outfile.open("w", encoding="utf-8") as f:
                for key, text in sections.items():
                    f.write(f"--- {key.upper()} ---\n{text}\n\n")
            print("‚úÖ Raw sections saved ‚Üí", outfile.absolute())
            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"test.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("\n‚úÖ Saved ‚Üí", outfile.absolute())


# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("‚ùå Fatal:", e)