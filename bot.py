# medical_ollama.py  ‚Äî  —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å Ollama (REST API)
from __future__ import annotations

import os
import re
import json
from pathlib import Path
from typing import Dict, List, TextIO
import fitz
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pandas as pd

import ollama  # pip install ollama
import logging
from datetime import datetime

# ---------- CONSTANTS ----------
OLLAMA_MODEL: str = "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q8_0"          # —Å–Ω–∞—á–∞–ª–∞ ¬´ollama pull <n>¬ª
SERVICES_FILE: Path = Path("docs/services.xlsx")  # Excel —Ñ–∞–π–ª —Å —É—Å–ª—É–≥–∞–º–∏
CHUNK_SIZE: int = 1_000
CHUNK_OVERLAP: int = 200
MAX_INPUT_TOK: int = 135_000

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Å–ª—É–≥
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('services_debug.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
services_logger = logging.getLogger('services_generation')
# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
SECTION_PATTERNS = [
    re.compile(r"^(\d+(?:\.\d+)*)\s+([^\n]+)", re.MULTILINE),  # 1.1, 2.3.4, ‚Ä¶ + title
    re.compile(r"^([IVX]+)\.\s+([^\n]+)", re.MULTILINE),       # I., II., III. + title
    re.compile(r"^(\d+)\.\s+([–ê-–Ø–Å][^\n]+)", re.MULTILINE),   # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
    re.compile(r"^([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+)*)\s*\n", re.MULTILINE),  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
    # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    re.compile(r"^([–ê-–Ø–Å]{2,}(?:\s+[–ê-–Ø–Å]{2,})*)\s*\n(?=[–ê-–Ø–Å–∞-—è—ë])", re.MULTILINE),  # –ó–ê–ì–û–õ–û–í–ö–ò –ó–ê–ì–õ–ê–í–ù–´–ú–ò
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏–∞–≥–Ω–æ–∑–∞
DIAGNOSIS_PATTERNS = [
    re.compile(r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç[^\n]*)", re.I),
    re.compile(r"(–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è[^\n]*)", re.I),
    re.compile(r"(–ê—Å—Ç–º–∞[^\n]*)", re.I),
    re.compile(r"(–ê—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è[^\n]*)", re.I),
    re.compile(r"(–ò—à–µ–º–∏—á–µ—Å–∫–∞—è –±–æ–ª–µ–∑–Ω—å —Å–µ—Ä–¥—Ü–∞[^\n]*)", re.I),
    re.compile(r"(–•—Ä–æ–Ω–∏—á–µ—Å–∫–∞—è –æ–±—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –±–æ–ª–µ–∑–Ω—å –ª–µ–≥–∫–∏—Ö[^\n]*)", re.I),
    re.compile(r"(–ü–Ω–µ–≤–º–æ–Ω–∏—è[^\n]*)", re.I),
    re.compile(r"(–ò–Ω—Ñ–∞—Ä–∫—Ç –º–∏–æ–∫–∞—Ä–¥–∞[^\n]*)", re.I),
    re.compile(r"(–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏[^\n]*)", re.I),
]
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self) -> None:
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        # –£–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —É—Å–ª—É–≥, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ –ò–ò
        # self.embeddings = self._lazy_embedder()
        # self.db = self._load_or_build_index()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É—Å–ª—É–≥–∏ –∏–∑ Excel —Ñ–∞–π–ª–∞
        self.services_df = self._load_services_from_excel()

    def _load_services_from_excel(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É—Å–ª—É–≥–∏ –∏–∑ Excel —Ñ–∞–π–ª–∞"""
        try:
            if not SERVICES_FILE.exists():
                print(f"‚ùå –§–∞–π–ª —Å —É—Å–ª—É–≥–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {SERVICES_FILE}")
                return pd.DataFrame(columns=['–ù–∞–∑–≤–∞–Ω–∏–µ', 'ID'])
            
            # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª
            df = pd.read_excel(SERVICES_FILE, sheet_name='iblock_element_admin (1)')
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            df.columns = ['–ù–∞–∑–≤–∞–Ω–∏–µ', 'ID']
            
            # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            df = df.dropna(subset=['–ù–∞–∑–≤–∞–Ω–∏–µ'])
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —É—Å–ª—É–≥ –∏–∑ Excel —Ñ–∞–π–ª–∞")
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É—Å–ª—É–≥ –∏–∑ Excel: {e}")
            return pd.DataFrame(columns=['–ù–∞–∑–≤–∞–Ω–∏–µ', 'ID'])



    # ---------- PDF PROCESSING ----------
    def _extract_text_with_fallback(self, pdf_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        try:
            # –ú–µ—Ç–æ–¥ 1: PyMuPDF (fitz)
            doc = fitz.open(pdf_path)
            text_parts = []
            
            for page_num, page in enumerate(doc):
                try:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
                    text = page.get_text("text")
                    
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    if len(text.strip()) < 100:
                        print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
                        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å OCR, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        # text = self._ocr_page(page)
                    
                    text_parts.append(text)
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
                    continue
            
            doc.close()
            full_text = "\n".join(text_parts)
            
            if len(full_text.strip()) < 500:
                print("–ò–∑–≤–ª–µ—á–µ–Ω–æ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥")
                return self._extract_with_langchain(pdf_path)
                
            return full_text
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ PyMuPDF: {e}")
            return self._extract_with_langchain(pdf_path)

    def _extract_with_langchain(self, pdf_path: str) -> str:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LangChain"""
        try:
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            return "\n".join(doc.page_content for doc in documents)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LangChain PDF loader: {e}")
            return ""

    def _extract_first_page(self, pdf_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF"""
        try:
            doc = fitz.open(pdf_path)
            if len(doc) > 0:
                first_page = doc[0]
                text = first_page.get_text("text")
                doc.close()
                return text
            doc.close()
            return ""
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            # Fallback —á–µ—Ä–µ–∑ LangChain
            try:
                from langchain_community.document_loaders import PyPDFLoader
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                if documents:
                    return documents[0].page_content
            except Exception as e2:
                print(f"–û—à–∏–±–∫–∞ fallback –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e2}")
            return ""

    def _is_valid_section_title(self, title: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞–ª–∏–¥–Ω—ã–º —Ä–∞–∑–¥–µ–ª–æ–º"""
        if not title or len(title.strip()) < 3:
            return False
            
        title_lower = title.lower().strip()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –≥–æ–¥—ã –∏ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        if re.match(r'^\d{4}\s*–≥\.?\s*', title_lower):
            return False
            
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –≥–æ–¥–∞–º–∏
        if '–≥.' in title_lower and any(year in title_lower for year in ['2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']):
            return False
            
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤
        if title.isupper() and len(title) < 10:
            return False
            
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if any(word in title_lower for word in ['–ø—Ä–µ–≤—ã—Å–∏–ª–∞', '—Å–æ—Å—Ç–∞–≤–∏–ª–∞', '—É–≤–µ–ª–∏—á–∏–ª–∞—Å—å', '—Å–Ω–∏–∑–∏–ª–∞—Å—å', '%', '–ø—Ä–æ—Ü–µ–Ω—Ç']):
            return False
            
        # –†–∞–∑—Ä–µ—à–∞–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Ä–∞–∑–¥–µ–ª—ã
        medical_keywords = [
            '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—ç—Ç–∏–æ–ª–æ–≥–∏—è', '–ø–∞—Ç–æ–≥–µ–Ω–µ–∑', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', 
            '–ª–µ—á–µ–Ω–∏–µ', '—Ç–µ—Ä–∞–ø–∏—è', '–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞', '–ø—Ä–æ–≥–Ω–æ–∑', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏',
            '–ø–æ–∫–∞–∑–∞–Ω–∏—è', '–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è', '–¥–æ–∑–∏—Ä–æ–≤–∫–∞', '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', '–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è'
        ]
        
        if any(keyword in title_lower for keyword in medical_keywords):
            return True
            
        # –†–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Ä–∞–∑–¥–µ–ª–æ–≤
        if re.match(r'^\d+\.?\d*\s+[–∞-—è—ë]', title_lower):
            return True
            
        # –†–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Å—Ç—Ä–æ—á–Ω—ã–µ
        if re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]', title) and len(title) > 5:
            return True
            
        return False

    def _extract_diagnosis_name(self, text: str, pdf_path: str = None) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞ –∏ –∫–æ–¥ –ú–ö–ë-10 —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ PDF, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–º–µ–Ω–Ω–æ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if pdf_path:
            first_page = self._extract_first_page(pdf_path)
            print(f"DEBUG: –ò–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({len(first_page)} —Å–∏–º–≤–æ–ª–æ–≤)")
        else:
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            first_page = text[:2000]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–¥–∞ –ú–ö–ë-10
        mkb_patterns = [
            re.compile(r'([A-Z]\d{2}(?:\.\d{1,2})?)', re.I),  # A00, B12.3, C45.1
            re.compile(r'–ú–ö–ë[- ]?10?[:\s]*([A-Z]\d{2}(?:\.\d{1,2})?)', re.I),
            re.compile(r'–∫–æ–¥[:\s]*([A-Z]\d{2}(?:\.\d{1,2})?)', re.I),
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏–∞–≥–Ω–æ–∑–∞
        diagnosis_patterns = [
            # –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ...
            re.compile(r'–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ\s+—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\s+(?:–ø–æ\s+)?(.+?)(?:\n|$)', re.I),
            # –ü—Ä–æ—Ç–æ–∫–æ–ª –≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å–Ω—ã—Ö...
            re.compile(r'–ø—Ä–æ—Ç–æ–∫–æ–ª\s+–≤–µ–¥–µ–Ω–∏—è\s+–±–æ–ª—å–Ω—ã—Ö\s+(.+?)(?:\n|$)', re.I),
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –ø–æ–º–æ—â–∏...
            re.compile(r'—Å—Ç–∞–Ω–¥–∞—Ä—Ç\s+–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π\s+–ø–æ–º–æ—â–∏\s+(?:–ø—Ä–∏\s+)?(.+?)(?:\n|$)', re.I),
            # –ê–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è...
            re.compile(r'–∞–ª–≥–æ—Ä–∏—Ç–º\s+(?:–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\s+–∏\s+)?–ª–µ—á–µ–Ω–∏—è\s+(.+?)(?:\n|$)', re.I),
            # –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...
            re.compile(r'–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ\s+—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\s+(?:–ø–æ\s+)?(.+?)(?:\n|$)', re.I),
            # –ü—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–æ–ª–µ–∑–Ω–∏ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            re.compile(r'^([–ê-–Ø–Å][–∞-—è—ë\s]+(?:–±–æ–ª–µ–∑–Ω—å|—Å–∏–Ω–¥—Ä–æ–º|–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ|–ø–∞—Ç–æ–ª–æ–≥–∏—è|–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å|–≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è|–¥–∏–∞–±–µ—Ç|–∞—Å—Ç–º–∞|–ø–Ω–µ–≤–º–æ–Ω–∏—è|–∏–Ω—Ñ–∞—Ä–∫—Ç|—Å—Ç–µ–Ω–æ–∫–∞—Ä–¥–∏—è|–∞—Ä–∏—Ç–º–∏—è|—Ç–∞—Ö–∏–∫–∞—Ä–¥–∏—è|–±—Ä–∞–¥–∏–∫–∞—Ä–¥–∏—è)[–∞-—è—ë\s]*)', re.M),
        ]
        
        mkb_code = None
        diagnosis_name = None
        
        # –ò—â–µ–º –∫–æ–¥ –ú–ö–ë-10
        for pattern in mkb_patterns:
            matches = pattern.findall(first_page)
            if matches:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–¥
                mkb_code = matches[0].upper()
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–¥ –ú–ö–ë-10: {mkb_code}")
                break
        
        # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞
        for pattern in diagnosis_patterns:
            match = pattern.search(first_page)
            if match:
                diagnosis_name = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
                diagnosis_name = re.sub(r'\s+', ' ', diagnosis_name)
                diagnosis_name = re.sub(r'["\'\(\)\[\]{}]', '', diagnosis_name)
                diagnosis_name = diagnosis_name.strip(' .,;:')
                
                if len(diagnosis_name) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞: {diagnosis_name}")
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ, –∏—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if not diagnosis_name:
            lines = first_page.split('\n')
            for line in lines[:15]:  # –ü–µ—Ä–≤—ã–µ 15 —Å—Ç—Ä–æ–∫
                line = line.strip()
                if (len(line) > 15 and len(line) < 200 and 
                    any(word in line.lower() for word in 
                        ['—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', '–ø—Ä–æ—Ç–æ–∫–æ–ª', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ']) and
                    not any(word in line.lower() for word in 
                        ['—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ', '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '–≥–ª–∞–≤–Ω—ã–π', '–≤—Ä–∞—á'])):
                    diagnosis_name = line
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {diagnosis_name}")
                    break
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        if mkb_code and diagnosis_name:
            result = f"{diagnosis_name} ({mkb_code})"
        elif diagnosis_name:
            result = diagnosis_name
        elif mkb_code:
            result = f"–ó–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ —Å –∫–æ–¥–æ–º {mkb_code}"
        else:
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –ª—é–±–æ–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            medical_terms = re.findall(r'([–ê-–Ø–Å][–∞-—è—ë\s]{10,80}(?:–±–æ–ª–µ–∑–Ω—å|—Å–∏–Ω–¥—Ä–æ–º|–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ|–ø–∞—Ç–æ–ª–æ–≥–∏—è|–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å|–≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è|–¥–∏–∞–±–µ—Ç|–∞—Å—Ç–º–∞|–ø–Ω–µ–≤–º–æ–Ω–∏—è|–∏–Ω—Ñ–∞—Ä–∫—Ç))', first_page)
            if medical_terms:
                result = medical_terms[0].strip()
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω: {result}")
            else:
                result = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
                print("‚ö†Ô∏è –î–∏–∞–≥–Ω–æ–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        return result

    def _parse_sections_advanced(self, text: str) -> Dict[str, str]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        sections = {}
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in SECTION_PATTERNS:
            splits = pattern.split(text)
            if len(splits) > 3:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª—ã
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern.pattern}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
                for i in range(1, len(splits), 3):
                    if i + 2 < len(splits):
                        number = splits[i].strip()
                        title = splits[i + 1].strip()
                        body = splits[i + 2].strip()
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        if self._is_valid_section_title(title):
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á
                            if number and title:
                                key = f"{number} {title}".strip()
                            elif title:
                                key = title
                            else:
                                continue
                                
                            # –û—á–∏—â–∞–µ–º —Ç–µ–ª–æ —Ä–∞–∑–¥–µ–ª–∞
                            if body and len(body) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                                sections[key] = body
                
                if sections:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω
                    break
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π —Ä–∞–∑–¥–µ–ª
        if not sections:
            print("‚ö†Ô∏è –†–∞–∑–¥–µ–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç")
            sections["guidelines"] = text
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {len(sections)}")
        for key in list(sections.keys())[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {key}")
        
        return sections

    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        """
        Reads a Russian clinical guideline PDF and returns a dict:
            {"1.1 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "...", "1.2 –≠—Ç–∏–æ–ª–æ–≥–∏—è": "...", ...}
        Keeps Cyrillic headings intact.
        """
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å fallback –º–µ—Ç–æ–¥–∞–º–∏
        full_text = self._extract_text_with_fallback(pdf_path)
        
        if not full_text or len(full_text.strip()) < 100:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF")
            return {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞
        self.diagnosis_name = self._extract_diagnosis_name(full_text, pdf_path)
        print(f"‚úÖ –î–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}")

        # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–¥–µ–ª—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
        sections = self._parse_sections_advanced(full_text)
        
        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª "guidelines", –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ-–¥—Ä—É–≥–æ–º—É
        if len(sections) == 1 and "guidelines" in sections:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            text = sections["guidelines"]
            keyword_sections = {}
            
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = [
                ("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", r"(–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–¥–µ—Ñ–∏–Ω–∏—Ü–∏—è)"),
                ("–≠—Ç–∏–æ–ª–æ–≥–∏—è", r"(—ç—Ç–∏–æ–ª–æ–≥–∏—è|–ø—Ä–∏—á–∏–Ω—ã|—Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞)"),
                ("–ü–∞—Ç–æ–≥–µ–Ω–µ–∑", r"(–ø–∞—Ç–æ–≥–µ–Ω–µ–∑|–º–µ—Ö–∞–Ω–∏–∑–º)"),
                ("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", r"(–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è|—Ç–∏–ø—ã|–≤–∏–¥—ã)"),
                ("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", r"(–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞|–¥–∏–∞–≥–Ω–æ–∑|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)"),
                ("–õ–µ—á–µ–Ω–∏–µ", r"(–ª–µ—á–µ–Ω–∏–µ|—Ç–µ—Ä–∞–ø–∏—è|–ø—Ä–µ–ø–∞—Ä–∞—Ç—ã)"),
                ("–ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞", r"(–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞|–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ)"),
                ("–ü—Ä–æ–≥–Ω–æ–∑", r"(–ø—Ä–æ–≥–Ω–æ–∑|–∏—Å—Ö–æ–¥)"),
            ]
            
            for section_name, pattern in keywords:
                match = re.search(f"({pattern}.*?)(?=({'|'.join([p[1] for p in keywords])})|$)", 
                                text, re.I | re.DOTALL)
                if match:
                    keyword_sections[section_name] = match.group(1).strip()
            
            if keyword_sections:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {len(keyword_sections)}")
                sections = keyword_sections

        return sections

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text: str, max_tok: int) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO) -> None:


        content = sections.get("guidelines", "")
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        system_prompt = (
            "–¢—ã ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ! "
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            "–í–∫–ª—é—á–∏ –í–°–ï –¥–∞–Ω–Ω—ã–µ: —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω—é–∞–Ω—Å—ã. "
            "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —É–∫–∞–∂–∏ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è. "
            "–ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ ¬´–Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å¬ª / ¬´–Ω–µ –¥–µ–ª–∞—Ç—å¬ª ‚Äî –≤–∫–ª—é—á–∏ —ç—Ç–æ. "
            "–ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –æ–¥–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏—Å–∫–ª—é—á–∞–µ—Ç –¥—Ä—É–≥–æ–µ - –≤–∫–ª—é—á–∏ —ç—Ç–æ –∏ —Ä–∞—Å–ø–∏—à–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ—Å—Ç —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∞—Ö–∞—Ä–Ω–æ–º –¥–∏–∞–±–µ—Ç–µ. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã. "
            "–í—Å–µ –æ—Ç–≤–µ—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. "
            "–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ c —à–∏—Ä–æ–∫–∏–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏, –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø–æ –¥–µ–ª—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –ª–µ—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞. "
            "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–µ–∑ –≤–≤–æ–¥–Ω—ã—Ö –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –æ—Ç —Ç–µ–±—è. "
            "–í–µ—Å—å –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!"
        )
        user_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ—Å—Ç—Ä–æ–π **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π –∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π** –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—é ¬´{self.diagnosis_name}¬ª.

        –í–ê–ñ–ù–û: –í–µ—Å—å –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –í–∫–ª—é—á–∏ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, —Å—Ä–æ–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è.
        - –£–∫–∞–∂–∏ **–≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏**, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.
        - –í–∫–ª—é—á–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –ø—Ä—è–º–æ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –∑–∞–ø—Ä–µ—Ç—ã/–æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π —Å—Ç—Ä–æ–≥–æ –ø–æ –¥–∞–Ω–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º:

        ### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
        - –≠–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è, —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        - –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        ### II. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (–∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä)
        2. –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–≤—Å–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –Ω–æ—Ä–º—ã, –∏—Å–∫–ª—é—á–µ–Ω–∏—è)
        3. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        4. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        5. –ö–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å–∫–ª—é—á–∞—é—Ç –¥—Ä—É–≥–∏–µ –∏ –≤ –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)

        ### III. –õ–µ—á–µ–Ω–∏–µ
        1. –ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
        2. –ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è (–ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Ä–µ–∂–∏–º, –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏)
        3. –ü—Ä–æ—Ü–µ–¥—É—Ä—ã / –æ–ø–µ—Ä–∞—Ü–∏–∏ / –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (—á–∞—Å—Ç–æ—Ç–∞, —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)

        ### IV. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        - –°—Ä–æ–∫–∏, –º–µ—Ç–æ–¥—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–∞

        ### V. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞
        - –û—Å–ª–æ–∂–Ω–µ–Ω–∏—è, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è, —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        - –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –∏ –¥–∏—Å–ø–∞–Ω—Å–µ—Ä–∏–∑–∞—Ü–∏—è

        ---

        **–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
        {content}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
                options={
                    "temperature": 0.1, 
                    "top_p": 0.95,
                    "num_predict": -1,
                    "stop": ["<|im_end|>", "</s>"],
                    "system": "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!"
                }
            )
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
        except Exception as e:
            print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama:", e)

    # ---------- SERVICES SELECTION ----------
    def generate_services_for_step(self, step_text: str, step_title: str = "") -> List[Dict[str, str]]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —É—Å–ª—É–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        services_logger.info(f"=== –ù–ê–ß–ê–õ–û –ì–ï–ù–ï–†–ê–¶–ò–ò –£–°–õ–£–ì ===")
        services_logger.info(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ —ç—Ç–∞–ø–∞: '{step_title}'")
        services_logger.info(f"–¢–µ–∫—Å—Ç —ç—Ç–∞–ø–∞ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): '{step_text[:200]}...'")
        services_logger.info(f"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(step_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if self.services_df is None or self.services_df.empty:
            services_logger.warning("–£—Å–ª—É–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ DataFrame –ø—É—Å—Ç–æ–π")
            return []
        
        services_logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —É—Å–ª—É–≥ –≤ DataFrame: {len(self.services_df)}")

        # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - –∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        medical_keywords = ['—É–∑–∏', '–∞–Ω–∞–ª–∏–∑', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', '–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—Ç–µ—Å—Ç', '–ø—Ä–æ–±–∞', '—Ä–µ–Ω—Ç–≥–µ–Ω', '—Ç–æ–º–æ–≥—Ä–∞—Ñ–∏—è', '—ç–Ω–¥–æ—Å–∫–æ–ø–∏—è', '–±–∏–æ–ø—Å–∏—è', '–ø—É–Ω–∫—Ü–∏—è', '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', '—Å–∫—Ä–∏–Ω–∏–Ω–≥']
        text_lower = step_text.lower()
        
        services_logger.info(f"–ü–æ–∏—Å–∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ...")
        found_keywords = [kw for kw in medical_keywords if kw in text_lower]
        services_logger.info(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {found_keywords}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        has_medical_terms = any(keyword in text_lower for keyword in medical_keywords)
        has_sufficient_content = len(step_text.strip()) > 10
        
        services_logger.info(f"–ï—Å—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: {has_medical_terms}")
        services_logger.info(f"–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {has_sufficient_content}")
        
        if not (has_medical_terms or has_sufficient_content):
            services_logger.warning("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞ - –Ω–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            return []

        exclude_patterns = [
            '–≤–≤–µ–¥–µ–Ω–∏–µ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è', '—ç—Ç–∏–æ–ª–æ–≥–∏—è', '–ø–∞—Ç–æ–≥–µ–Ω–µ–∑',
            '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',
            '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '–∫—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞', '–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è'
        ]

        services_logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–∞—é—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        for pattern in exclude_patterns:
            if pattern in step_title.lower():
                services_logger.warning(f"–ù–∞–π–¥–µ–Ω –∏—Å–∫–ª—é—á–∞—é—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω '{pattern}' –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return []
        
        services_logger.info("–ò—Å–∫–ª—é—á–∞—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        services_logger.info("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —É—Å–ª—É–≥ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å...")
        services_list = self.services_df.head(1000).apply(
            lambda row: f"{row['ID']} - {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}", axis=1
        ).tolist()
        
        services_logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(services_list)} —É—Å–ª—É–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        services_text = "\n".join(services_list)
        services_logger.info(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å —É—Å–ª—É–≥–∞–º–∏: {len(services_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        services_logger.info("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞...")
        system_prompt = (
            "–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤–æ–º –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π!\n\n"
            "–ó–∞–¥–∞—á–∞: –Ω–∞–π—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Å–ª—É–≥–∏ –¥–ª—è —ç—Ç–∞–ø–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.\n\n"
            "–ü–†–ê–í–ò–õ–ê:\n"
            "1. –û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ: [{\"id\": \"123\", \"name\": \"–ù–∞–∑–≤–∞–Ω–∏–µ\"}]\n"
            "2. –ú–∞–∫—Å–∏–º—É–º 5 —É—Å–ª—É–≥\n"
            "3. –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö ‚Äî –≤–µ—Ä–Ω–∏: []\n"
            "4. –ù–ï –ü–ò–®–ò –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n"
            "5. –¢–û–õ–¨–ö–û JSON!\n\n"
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON-–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏:\n"
            "- id: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å–ª—É–≥–∏\n"
            "- name: –Ω–∞–∑–≤–∞–Ω–∏–µ —É—Å–ª—É–≥–∏\n\n"
            "–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:\n"
            '[{\"id\": \"123\", \"name\": \"–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏\"}]\n\n'
            '[{\"id\": \"456\", \"name\": \"–£–ó–ò –±—Ä—é—à–Ω–æ–π –ø–æ–ª–æ—Å—Ç–∏\"}]\n\n'
            '[]\n\n'
            f"–£—Å–ª—É–≥–∏:\n{services_text}\n\n"
            f"–≠—Ç–∞–ø: {step_title}\n"
            f"–û–ø–∏—Å–∞–Ω–∏–µ: {step_text[:1500]}\n\n"
            "JSON-–æ—Ç–≤–µ—Ç:"
        )
        
        services_logger.info(f"–†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        services_logger.info(f"–û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —ç—Ç–∞–ø–∞: '{step_text[:2000]}'")

        try:
            services_logger.info("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ Ollama...")
            services_logger.info(f"–ú–æ–¥–µ–ª—å: {OLLAMA_MODEL}")
            
            response = ollama.chat(
                model=OLLAMA_MODEL,
                messages=[{"role": "system", "content": system_prompt}],
                options={
                    "temperature": 0.0,
                    "top_p": 0.05,
                    "top_k": 10,
                    "num_predict": 200,
                    "repeat_penalty": 1.1,
                    "stop": ["\n\n", "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:", "–ê–Ω–∞–ª–∏–∑:", "---"]
                }
            )
            
            services_logger.info("–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
            raw = response['message']['content'].strip()
            services_logger.info(f"–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: '{raw}'")
            services_logger.info(f"–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(raw)} —Å–∏–º–≤–æ–ª–æ–≤")

            # üîç –ò—â–µ–º JSON-–º–∞—Å—Å–∏–≤
            services_logger.info("–ü–æ–∏—Å–∫ JSON-–º–∞—Å—Å–∏–≤–∞ –≤ –æ—Ç–≤–µ—Ç–µ...")
            match = re.search(r'\[.*?\]', raw, re.DOTALL)
            if match:
                json_str = match.group(0)
                services_logger.info(f"–ù–∞–π–¥–µ–Ω JSON: '{json_str}'")
                try:
                    result = json.loads(json_str)
                    services_logger.info(f"JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ —É—Å–ª—É–≥: {len(result)}")
                    for i, service in enumerate(result):
                        services_logger.info(f"–£—Å–ª—É–≥–∞ {i+1}: ID={service.get('id', 'N/A')}, –ù–∞–∑–≤–∞–Ω–∏–µ='{service.get('name', 'N/A')}'")
                    services_logger.info("=== –ö–û–ù–ï–¶ –ì–ï–ù–ï–†–ê–¶–ò–ò –£–°–õ–£–ì ===\n")
                    return result
                except json.JSONDecodeError as je:
                    services_logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {je}")
            else:
                services_logger.warning("JSON-–º–∞—Å—Å–∏–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
                
        except Exception as e:
            services_logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Å–ª—É–≥: {e}")
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ JSON: {e}")

        services_logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
        services_logger.info("=== –ö–û–ù–ï–¶ –ì–ï–ù–ï–†–ê–¶–ò–ò –£–°–õ–£–ì ===\n")
        return []

    # ---------- DIALOGUE ----------
    def _generate_dialogue_streaming(self, user_message: str, conversation_history: List[Dict], sections: Dict[str, str], file: TextIO) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        print(f"DEBUG: –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è: {user_message[:100]}...")
        
        content = sections.get("guidelines", "")
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        system_prompt = (
             "–í–ê–ñ–ù–û: –ì–ï–ù–ï–†–ò–†–£–ô –¢–ï–ö–°–¢ –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ! "
            "–¢—ã ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. "
            "–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ! "
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            "–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö. "
            "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º. "
            "–ë—É–¥—å —Ç–æ—á–Ω—ã–º, –≤–∫–ª—é—á–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞, –¥–æ–∑—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, —Å—Ä–æ–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. "
            "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥, –æ—Ç–≤–µ—á–∞–π –Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã. "
            "–í–µ—Å—å –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!"
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞
        messages = [{"role": "system", "content": system_prompt}]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        context_message = f"""
        –î–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}
        
        –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        {content}
        """
        messages.append({"role": "system", "content": context_message})
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        for msg in conversation_history:
            messages.append(msg)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        messages.append({"role": "user", "content": user_message})

        try:
            print(f"DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                ollama.list()
                print("DEBUG: Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
            except Exception as e:
                print(f"DEBUG: Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                error_msg = "‚ùå –û—à–∏–±–∫–∞: Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
                file.write(error_msg)
                return
            
            print(f"DEBUG: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ {OLLAMA_MODEL}...")
            
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
                options={
                    "temperature": 0.6, 
                    "top_p": 0.95,
                    "num_predict": -1,
                    "stop": ["<|im_end|>", "</s>"],
                    "system": "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!"
                }
            )
            
            print("DEBUG: –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏...")
            token_count = 0
            
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
                token_count += 1
                
            print(f"\nDEBUG: –ü–æ–ª—É—á–µ–Ω–æ {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama: {str(e)}"
            print(f"\nDEBUG: {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
            if "Connection refused" in str(e) or "connection" in str(e).lower():
                error_msg = "‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω (–∫–æ–º–∞–Ω–¥–∞: ollama serve)"
            elif "model" in str(e).lower():
                error_msg = f"‚ùå –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å {OLLAMA_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–∫–æ–º–∞–Ω–¥–∞: ollama pull {OLLAMA_MODEL})"
            elif "tunnel" in str(e).lower():
                error_msg = "‚ùå –û—à–∏–±–∫–∞ —Ç—É–Ω–Ω–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Ç–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"
            
            file.write(error_msg)

    # ---------- RUN ----------
    def run(self):
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit", "q"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue

            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            outfile = Path("testSec.txt")
            with outfile.open("w", encoding="utf-8") as f:
                for key, text in sections.items():
                    f.write(f"--- {key.upper()} ---\n{text}\n\n")
            print("‚úÖ Raw sections saved ‚Üí", outfile.absolute())
            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"test.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("\n‚úÖ Saved ‚Üí", outfile.absolute())


# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("‚ùå Fatal:", e)