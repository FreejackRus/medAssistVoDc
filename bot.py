import os
import re
import pandas as pd
from typing import List, Dict, Optional
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
from llama_cpp import Llama
from langchain.text_splitter import RecursiveCharacterTextSplitter


class MedicalAssistant:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        self.db = self.init_knowledge_base()
        self.llm = self.load_gguf_model()
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        self.current_guidelines = {}
        self.diagnosis_name = ""

    def load_gguf_model(self) -> Llama:
        """–ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ from_pretrained"""
        try:
            llm = Llama.from_pretrained(
                repo_id="unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF",
                filename="DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
                n_ctx=20000,
                n_threads=8,
                n_gpu_layers=40,
                temperature=0.3,
                top_p=0.95,
                repeat_penalty=1.2,
                verbose=False
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face Hub")
            return llm
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            raise

    def init_knowledge_base(self) -> FAISS:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        documents = []

        # –£—Å–ª—É–≥–∏
        print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ —É—Å–ª—É–≥...")
        documents.extend(self.load_services_documents())

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
        try:
            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            return FAISS.from_documents(documents, embeddings)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {str(e)}")
            raise

    def load_services_documents(self) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —É—Å–ª—É–≥–∏ –∏–∑ Excel"""
        try:
            df = pd.read_excel("docs/services.xlsx")
            required_columns = ["ID", "–ù–∞–∑–≤–∞–Ω–∏–µ"]
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"–í —Ñ–∞–π–ª–µ services.xlsx –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required_columns}")

            documents = []
            for _, row in df.iterrows():
                service_id = str(row["ID"])
                service_name = str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"])
                content = f"–£—Å–ª—É–≥–∞ {service_id}: {service_name}"
                metadata = {
                    "source": "services",
                    "id": service_id,
                    "name": service_name,
                    "type": "medical_service"
                }
                documents.append(Document(page_content=content, metadata=metadata))

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} —É—Å–ª—É–≥")
            return documents
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É—Å–ª—É–≥: {str(e)}")
            return []

    def load_guidelines_from_pdf(self, pdf_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç PDF —Å –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
        try:
            loader = PyPDFLoader(pdf_path)
            pages = loader.load_and_split()
            full_text = "\n".join([page.page_content for page in pages])
            self.current_guidelines["full_text"] = full_text

            sections = {
                "diagnosis": self.extract_section(full_text, "–¥–∏–∞–≥–Ω–æ–∑", "–ª–µ—á–µ–Ω–∏–µ|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"),
                "treatment": self.extract_section(full_text, "–ª–µ—á–µ–Ω–∏–µ", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è|–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞"),
                "monitoring": self.extract_section(full_text, "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è|—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"),
                "complications": self.extract_section(full_text, "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
            }

            title_match = re.search(r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n", full_text[:1000], re.IGNORECASE)
            self.diagnosis_name = title_match.group(1).strip() if title_match else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"

            print(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}")
            return sections
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF: {str(e)}")
            return {}

    def extract_section(self, text: str, start_marker: str, end_marker: str) -> str:
        """–í—ã–¥–µ–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º"""
        try:
            pattern_start = re.compile(start_marker, re.IGNORECASE)
            pattern_end = re.compile(end_marker, re.IGNORECASE)

            start_match = pattern_start.search(text)
            if not start_match:
                return ""

            start_idx = start_match.start()
            end_match = pattern_end.search(text, start_idx + len(start_marker))
            end_idx = end_match.start() if end_match else len(text)

            return text[start_idx:end_idx]
        except:
            return ""

    def find_relevant_services(self, query: str, k: Optional[int] = None) -> List[Document]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —É—Å–ª—É–≥–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
        try:
            services = self.safe_similarity_search(query, k=k or 50, source_filter="services", fetch_k=(k or 50) * 3)
            seen_ids = set()
            unique_services = []
            for service in services:
                service_id = service.metadata.get("id")
                if service_id and service_id not in seen_ids:
                    seen_ids.add(service_id)
                    unique_services.append(service)
            return unique_services
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —É—Å–ª—É–≥: {str(e)}")
            return []

    def safe_similarity_search(self, query: str, k: int = 3, source_filter: Optional[str] = None, fetch_k: int = 50) -> List[Document]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∏—Å–∫ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            all_docs = []
            for doc_id in self.db.index_to_docstore_id.values():
                doc = self.db.docstore.search(doc_id)
                all_docs.append(doc)

            filtered_docs = []
            for doc in all_docs:
                if isinstance(doc, Document):
                    if not source_filter or doc.metadata.get("source") == source_filter:
                        filtered_docs.append(doc)
                elif isinstance(doc, dict):
                    if not source_filter or doc.get("metadata", {}).get("source") == source_filter:
                        filtered_docs.append(Document(page_content=doc.get("page_content", ""), metadata=doc.get("metadata", {})))

            if filtered_docs:
                embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                temp_db = FAISS.from_documents(filtered_docs, embeddings)
                return temp_db.similarity_search(query[:300], k=k, fetch_k=fetch_k)
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
            return []

    def generate_step_recommendations(self, step: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –ª–µ—á–µ–Ω–∏—è"""
        try:
            prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–æ—Å—Ç–∞–≤—å —á—ë—Ç–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –≤—Ä–∞—á–∞:

–≠—Ç–∞–ø: {step['step']}
–û–ø–∏—Å–∞–Ω–∏–µ: {step['description']}
–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
{step['content'][:3000]}

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:
1. –ö–ª—é—á–µ–≤—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏/–ø—Ä–∏–Ω—Ü–∏–ø—ã (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤)
2. –ê–ª–≥–æ—Ä–∏—Ç–º –¥–µ–π—Å—Ç–≤–∏–π (–ø–æ—à–∞–≥–æ–≤–æ)
3. –ò—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
4. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è/–º–µ—Ç–æ–¥—ã
5. –í–∞–∂–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è/–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

–§–æ—Ä–º–∞—Ç: –∫—Ä–∞—Ç–∫–æ, –ø–æ –ø—É–Ω–∫—Ç–∞–º, –±–µ–∑ –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤.
"""

            response = self.llm.create_chat_completion(messages=[{"role": "user", "content": prompt}], max_tokens=2000)
            recommendations = response['choices'][0]['message']['content']

            services = self.find_relevant_services(step['description'])
            return {
                "step": step["step"],
                "recommendations": recommendations.strip(),
                "services": services
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {str(e)}")
            return {"step": step["step"], "recommendations": "", "services": []}

    def build_treatment_algorithm(self) -> List[Dict]:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è"""
        algorithm = []

        if self.current_guidelines.get("diagnosis"):
            algorithm.append({
                "step": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞",
                "description": "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–∏–∞–≥–Ω–æ–∑–∞ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π",
                "content": self.current_guidelines["diagnosis"]
            })

        if self.current_guidelines.get("treatment"):
            algorithm.append({
                "step": "–û—Å–Ω–æ–≤–Ω–æ–µ –ª–µ—á–µ–Ω–∏–µ",
                "description": "–í—ã–±–æ—Ä —Ç–∞–∫—Ç–∏–∫–∏, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ—Ä–∞–ø–∏–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π",
                "content": self.current_guidelines["treatment"]
            })

        if self.current_guidelines.get("monitoring"):
            algorithm.append({
                "step": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
                "description": "–ö–æ–Ω—Ç—Ä–æ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏",
                "content": self.current_guidelines["monitoring"]
            })

        if self.current_guidelines.get("complications"):
            algorithm.append({
                "step": "–û—Å–ª–æ–∂–Ω–µ–Ω–∏—è",
                "description": "–ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –∏ –ª–µ—á–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Å–ª–æ–∂–Ω–µ–Ω–∏–π",
                "content": self.current_guidelines["complications"]
            })

        return algorithm

    def format_output(self, algorithm: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        output = []
        output.append(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ {self.diagnosis_name} (–Ω–∞ –æ—Å–Ω–æ–≤–µ PDF)")
        for step in algorithm:
            output.append(f"\n{'=' * 60}")
            output.append(f"ü©∫ –®–ê–ì: {step['step']}")
            output.append(f"üîç –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:\n{step['description']}")
            output.append(f"\nüìù –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n{step['recommendations']}")

            output.append(f"\nü©∫ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –£–°–õ–£–ì–ò:")
            if step["services"]:
                for i, service in enumerate(step["services"], 1):
                    name = service.metadata.get('name', '‚Äî')
                    desc = service.metadata.get('description', '‚Äî')
                    output.append(f"{i}. {name} ‚Äî {desc}")
            else:
                output.append("‚ö†Ô∏è –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —É—Å–ª—É–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return "\n".join(output)

    def save_recommendations(self, content: str, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        if not filename:
            safe_name = re.sub(r'[^\w\s-]', '', self.diagnosis_name).strip().replace(' ', '_')[:50]
            filename = f"—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏_{safe_name}.txt"
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}")

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Ä–∞–±–æ—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        try:
            print("ü§ñ –ê–°–°–ò–°–¢–ï–ù–¢ –ü–û –ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ú –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø–ú")
            while True:
                pdf_path = input("üìÑ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
                if pdf_path.lower() in ['exit', '–≤—ã–π—Ç–∏', 'quit']:
                    break
                if not os.path.exists(pdf_path):
                    print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
                    continue

                print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF...")
                self.current_guidelines = self.load_guidelines_from_pdf(pdf_path)
                if not self.current_guidelines:
                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
                    continue

                algorithm_steps = self.build_treatment_algorithm()
                full_algorithm = []

                for step in algorithm_steps:
                    print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç—Ç–∞–ø–∞: {step['step']}")
                    full_algorithm.append(self.generate_step_recommendations(step))

                output = self.format_output(full_algorithm)
                self.save_recommendations(output)
                print("\n‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
        except KeyboardInterrupt:
            print("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == '__main__':
    try:
        assistant = MedicalAssistant()
        assistant.run()
    except Exception as e:
        print(f"‚ùå –§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
    finally:
        input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")