# medical_qwen.py  (YaRN-free, LangChain 0.2+)
from __future__ import annotations

import os
import re
import threading
import gc
from pathlib import Path
from typing import Dict, List, TextIO, Tuple

import pandas as pd
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TextStreamer,
    AutoConfig,
)
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from torch import cuda
from gpu_monitor import print_vram  # optional

# ------------------------- CONFIG -------------------------
HF_MODEL_NAME = "moonshotai/Kimi-K2-Instruct "
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE = Path("docs/services.xlsx")
CHUNK_SIZE = 1_000
CHUNK_OVERLAP = 200
MAX_NEW_TOKENS = 10_048  # safe for 24 GB
MAX_INPUT_TOK = 128_000  # safe for 24 GB
TOP_K = 50
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self):
        self._stop_monitor = threading.Event()
        self.device = "cuda" if cuda.is_available() else "cpu"
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        self.tokenizer, self.model = self._load_llm()
        self.db = self._load_or_build_index()

    # ---------- LLM ----------
    def _load_llm(self):
        print("‚åõ Loading Instruct (bf16, no YaRN)...")
        tok = AutoTokenizer.from_pretrained(
            HF_MODEL_NAME,
            trust_remote_code=True,
        )
        cfg = AutoConfig.from_pretrained(
            HF_MODEL_NAME,
            trust_remote_code=True,
        )
        # raise context limit if you really need it (optional)
        cfg.max_position_embeddings = 128000  # native 32 k

        mdl = AutoModelForCausalLM.from_pretrained(
            HF_MODEL_NAME,
            config=cfg,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        print("‚úÖ Model ready.")
        print_vram("After model")
        return tok, mdl

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": self.device},
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache",
        )

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent),
                self.embeddings,
                index_name=idx_path.stem,
                allow_dangerous_deserialization=True,
            )

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(
                page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                metadata={
                    "source": "services",
                    "id": str(row["ID"]),
                    "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]),
                },
            )
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF ----------
    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}
        pages = PyPDFLoader(pdf_path).load_and_split()
        full_text = "\n".join(p.page_content for p in pages)

        title = re.search(
            r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n",
            full_text[:2000],
            re.I,
        )
        self.diagnosis_name = (
            title.group(1).strip() if title else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
        )
        print(f"‚úÖ Diagnosis: {self.diagnosis_name}")

        def grab(txt, start, end):
            s = re.search(start, txt, re.I)
            if not s:
                return ""
            e = re.search(end, txt[s.end() :], re.I)
            return txt[
                s.start() : s.end() + (e.start() if e else len(txt))
            ].strip()

        return {
            "diagnosis": grab(full_text, "–¥–∏–∞–≥–Ω–æ–∑", "–ª–µ—á–µ–Ω–∏–µ|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"),
            "treatment": grab(full_text, "–ª–µ—á–µ–Ω–∏–µ", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è"),
            "monitoring": grab(
                full_text, "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è"
            ),
            "complications": grab(
                full_text, "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"
            ),
        }

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text, max_tok) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    def find_services(self, query, k=TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k * 3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO):
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(
            f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services
        )

        prompt = f"""
        –í—ã ‚Äî –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –¥–∏–∞–≥–Ω–æ–∑–∞ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Å–ª—É–≥–∏ —Å –∏—Ö ID. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω, –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –æ–∫–∞–∑–∞–Ω–∏—è –ø–æ–º–æ—â–∏.

        –î–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}
        –î–æ—Å—Ç—É–ø–Ω—ã–µ —É—Å–ª—É–≥–∏ (ID): {services}

        –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
        ### –ü–æ–¥—Ä–æ–±–Ω—ã–π –ê–ª–≥–æ—Ä–∏—Ç–º –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –õ–µ—á–µ–Ω–∏—è {self.diagnosis_name.split(':')[-1].strip()}

        #### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–∞
        - –ö—Ä–∞—Ç–∫–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–≥–Ω–æ–∑–∞
        - –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        - –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        #### II. –≠—Ç–∞–ø—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        1. **–ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ**:
           - –î–µ–π—Å—Ç–≤–∏—è –≤—Ä–∞—á–∞
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (ID —É—Å–ª—É–≥)
           - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        2. **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏**:
           - –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (ID —É—Å–ª—É–≥)
           - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è

        #### III. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        - –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        - –ü—Ä–∏–∑–Ω–∞–∫–∏, –æ—Ç–ª–∏—á–∞—é—â–∏–µ —Ç–µ–∫—É—â–∏–π –¥–∏–∞–≥–Ω–æ–∑
        - –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã

        #### IV. –¢–∞–∫—Ç–∏–∫–∞ –ª–µ—á–µ–Ω–∏—è
        1. **–ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è**:
           - –û–±—Ä–∞–∑ –∂–∏–∑–Ω–∏, –¥–∏–µ—Ç–∞, —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        2. **–ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è**:
           - –ü—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑–∏—Ä–æ–≤–∫–∏, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        3. **–ü—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏**:
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —É—Å–ª—É–≥–∏ (ID)
           - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è
        4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏**
           - –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
           - –ú–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏

        #### V. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é
        - –ü–ª–∞–Ω –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
        - –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ä—ã

        #### VI. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏ –æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è—Ö
        - –í–∞—Ä–∏–∞–Ω—Ç—ã –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏
        - –°–æ–≤–µ—Ç—ã –ø–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è–º–∏

        #### VII. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
        - –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        - –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏

        –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º ID —É—Å–ª—É–≥
        """
        content = "\n\n".join(
            f"--- {k.upper()} ---\n{v}" for k, v in sections.items() if v
        )
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        messages = [{"role": "user", "content": prompt + content}]
        chat = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,
        )
        inputs = self.tokenizer(chat, return_tensors="pt").to(self.model.device)

        streamer = TextStreamer(
            self.tokenizer, skip_prompt=True, skip_special_tokens=True
        )
        with torch.no_grad():
            generated = self.model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.3,
                do_sample=True,
                streamer=streamer,
                pad_token_id=self.tokenizer.eos_token_id,
            )

        answer = self.tokenizer.decode(
            generated[0][inputs.input_ids.shape[1] :],
            skip_special_tokens=True,
        )
        file.write(answer)

    # ---------- RUN ----------
    def run(self):
        if not (self.tokenizer and self.model and self.db):
            print("‚ùå Not initialized")
            return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue
            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            safe = (
                re.sub(r"[^\w\s-]", "", self.diagnosis_name)
                .strip()
                .replace(" ", "_")[:50]
            )
            outfile = Path(f"—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏_{safe}.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("‚úÖ Saved ‚Üí", outfile.absolute())


if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except Exception as e:
        print("‚ùå Fatal:", e)
    finally:
        input("Press Enter to exit")