# medical_ollama.py  ‚Äî  —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å Ollama (REST API)
from __future__ import annotations

import os
import re
from pathlib import Path
from typing import Dict, List, TextIO

import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import ollama  # pip install ollama

# ------------------------- CONFIG -------------------------
OLLAMA_MODEL: str = "kimi-k2-instruct"          # —Å–Ω–∞—á–∞–ª–∞ ¬´ollama pull <name>¬ª
EMBED_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE: Path = Path("docs/services.xlsx")
CHUNK_SIZE: int = 1_000
CHUNK_OVERLAP: int = 200
TOP_K: int = 50
MAX_INPUT_TOK: int = 128_000
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self) -> None:
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        self.db = self._load_or_build_index()

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": "cpu"},  # –º–æ–∂–Ω–æ "cuda", –µ—Å–ª–∏ –µ—Å—Ç—å
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache",
        )

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent),
                self.embeddings,
                index_name=idx_path.stem,
                allow_dangerous_deserialization=True,
            )

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(
                page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                metadata={
                    "source": "services",
                    "id": str(row["ID"]),
                    "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]),
                },
            )
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF ----------
    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}
        pages = PyPDFLoader(pdf_path).load_and_split()
        full_text = "\n".join(p.page_content for p in pages)

        title = re.search(
            r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n",
            full_text[:2000],
            re.I,
        )
        self.diagnosis_name = (
            title.group(1).strip() if title else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
        )
        print(f"‚úÖ Diagnosis: {self.diagnosis_name}")

        def grab(txt, start, end):
            s = re.search(start, txt, re.I)
            if not s:
                return ""
            e = re.search(end, txt[s.end() :], re.I)
            return txt[
                s.start() : s.end() + (e.start() if e else len(txt))
            ].strip()

        return {
            "diagnosis": grab(full_text, "–¥–∏–∞–≥–Ω–æ–∑", "–ª–µ—á–µ–Ω–∏–µ|–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"),
            "treatment": grab(full_text, "–ª–µ—á–µ–Ω–∏–µ", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è"),
            "monitoring": grab(
                full_text, "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥|–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è"
            ),
            "complications": grab(
                full_text, "–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"
            ),
        }

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text: str, max_tok: int) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    def find_services(self, query: str, k: int = TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k * 3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO) -> None:
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(
            f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services
        )

        prompt = f"""
–í—ã ‚Äî –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –¥–∏–∞–≥–Ω–æ–∑–∞ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Å–ª—É–≥–∏ —Å –∏—Ö ID. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω, –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –æ–∫–∞–∑–∞–Ω–∏—è –ø–æ–º–æ—â–∏.

        –î–∏–∞–≥–Ω–æ–∑: {self.diagnosis_name}
        –î–æ—Å—Ç—É–ø–Ω—ã–µ —É—Å–ª—É–≥–∏ (ID): {services}

        –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
        ### –ü–æ–¥—Ä–æ–±–Ω—ã–π –ê–ª–≥–æ—Ä–∏—Ç–º –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –õ–µ—á–µ–Ω–∏—è {self.diagnosis_name.split(':')[-1].strip()}

        #### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–∞
        - –ö—Ä–∞—Ç–∫–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–≥–Ω–æ–∑–∞
        - –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        - –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        #### II. –≠—Ç–∞–ø—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        1. **–ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ**:
           - –î–µ–π—Å—Ç–≤–∏—è –≤—Ä–∞—á–∞
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (ID —É—Å–ª—É–≥)
           - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        2. **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏**:
           - –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (ID —É—Å–ª—É–≥)
           - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è

        #### III. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        - –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        - –ü—Ä–∏–∑–Ω–∞–∫–∏, –æ—Ç–ª–∏—á–∞—é—â–∏–µ —Ç–µ–∫—É—â–∏–π –¥–∏–∞–≥–Ω–æ–∑
        - –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã

        #### IV. –¢–∞–∫—Ç–∏–∫–∞ –ª–µ—á–µ–Ω–∏—è
        1. **–ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è**:
           - –û–±—Ä–∞–∑ –∂–∏–∑–Ω–∏, –¥–∏–µ—Ç–∞, —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        2. **–ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è**:
           - –ü—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑–∏—Ä–æ–≤–∫–∏, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        3. **–ü—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏**:
           - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —É—Å–ª—É–≥–∏ (ID)
           - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è
        4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏**
           - –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
           - –ú–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏

        #### V. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é
        - –ü–ª–∞–Ω –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
        - –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ä—ã

        #### VI. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏ –æ—Å–ª–æ–∂–Ω–µ–Ω–∏—è—Ö
        - –í–∞—Ä–∏–∞–Ω—Ç—ã –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏
        - –°–æ–≤–µ—Ç—ã –ø–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è–º–∏

        #### VII. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
        - –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        - –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏

        –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º ID —É—Å–ª—É–≥
        """
        content = "\n\n".join(
            f"--- {k.upper()} ---\n{v}" for k, v in sections.items() if v
        )
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        messages = [{"role": "user", "content": prompt + content}]

        try:
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
            )
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
        except Exception as e:
            print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama:", e)

    # ---------- RUN ----------
    def run(self):
        if not self.db:
            print("‚ùå Index not loaded")
            return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit", "q"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue

            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"test.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("\n‚úÖ Saved ‚Üí", outfile.absolute())


# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("‚ùå Fatal:", e)